WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\maddpg\common\tf_util.py:163: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\maddpg\trainer\maddpg.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\contrib\layers\python\layers\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\python\ops\clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Using good policy maddpg and adv policy maddpg
Loading previous state...
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\experiments\algo\airl_discriminator.py:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\experiments\algo\airl_discriminator.py:26: The name tf.losses.log_loss is deprecated. Please use tf.compat.v1.losses.log_loss instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\python\training\moving_averages.py:434: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
## New start!
Starting iterations...
episode reward for DDPG agent: 7.587215737104416
steps: 4975, episodes: 200, mean episode reward: -4.784120378299446, time: 7.341
good reward is 8.346610000081961
Avg best coop dist to target 0.35646463155383384
episode reward for DDPG agent: 6.9882158386707305
steps: 9975, episodes: 400, mean episode reward: -5.687976196813323, time: 6.437
good reward is 6.697823355859576
Avg best coop dist to target 0.3579460656612667
episode reward for DDPG agent: 5.44048040509224
steps: 14975, episodes: 600, mean episode reward: -3.427024920200121, time: 7.59
good reward is 8.212427600880401
Avg best coop dist to target 0.33924434180937696
episode reward for DDPG agent: 0.4820253673195839
steps: 19975, episodes: 800, mean episode reward: -4.426737565617891, time: 8.698
good reward is 8.179343540613694
Avg best coop dist to target 0.3387373074973185
episode reward for DDPG agent: -4.869282056689262
steps: 24975, episodes: 1000, mean episode reward: -3.867494479742857, time: 8.679
good reward is 8.985013568219676
Avg best coop dist to target 0.3343403058669122
episode reward for DDPG agent: -10.337444584667683
steps: 29975, episodes: 1200, mean episode reward: -6.248950427295719, time: 12.081
good reward is 6.9766885184481495
Avg best coop dist to target 0.3417238645771592
episode reward for DDPG agent: -15.595158207416535
steps: 34975, episodes: 1400, mean episode reward: -3.3634799130917026, time: 11.385
good reward is 8.67267103283467
Avg best coop dist to target 0.3346507177658155
episode reward for DDPG agent: -20.8421157091856
steps: 39975, episodes: 1600, mean episode reward: -5.509439398110464, time: 11.43
good reward is 6.883348269469306
Avg best coop dist to target 0.3395693165059991
episode reward for DDPG agent: -31.404267240166664
steps: 44975, episodes: 1800, mean episode reward: -3.3116489009807175, time: 11.497
good reward is 10.444589897352335
Avg best coop dist to target 0.33496938582790886
episode reward for DDPG agent: -40.6856639996171
steps: 49975, episodes: 2000, mean episode reward: -3.839914797795394, time: 11.445
good reward is 9.578518364483676
Avg best coop dist to target 0.3329786500428165
final is 8.297703414824344
Traceback (most recent call last):
  File "C:/Users/liut18/Desktop/maddpg/experiments/train_DDPG_Adv_Policy_PD_Blackbox.py", line 386, in <module>
    train(arglist)
  File "C:/Users/liut18/Desktop/maddpg/experiments/train_DDPG_Adv_Policy_PD_Blackbox.py", line 374, in train
    rew_file_name = arglist.plots_dir + arglist.exp_name + '_rewards.pkl'
TypeError: must be str, not NoneType