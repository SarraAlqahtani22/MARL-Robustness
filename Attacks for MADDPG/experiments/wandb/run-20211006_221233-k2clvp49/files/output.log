WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\maddpg\common\tf_util.py:163: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\maddpg\trainer\maddpg.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\contrib\layers\python\layers\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\python\ops\clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Using good policy maddpg and adv policy maddpg
Loading previous state...
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\experiments\algo\airl_discriminator.py:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\experiments\algo\airl_discriminator.py:26: The name tf.losses.log_loss is deprecated. Please use tf.compat.v1.losses.log_loss instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\python\training\moving_averages.py:434: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
## New start!
loading-------------------------------------------------------------
Starting iterations...
episode reward for DDPG agent: 0.4697545489668846
steps: 4975, episodes: 200, mean episode reward: -4.711915984069716, time: 7.185
good reward is 8.486124858220519
Avg best coop dist to target 0.35901781048127096
episode reward for DDPG agent: -0.8361235934495926
steps: 9975, episodes: 400, mean episode reward: -5.798377342258693, time: 6.18
good reward is 6.776650104415657
Avg best coop dist to target 0.3605190740487456
episode reward for DDPG agent: 0.23435446947813035
steps: 14975, episodes: 600, mean episode reward: -3.907699831348074, time: 6.158
good reward is 9.211802519777212
Avg best coop dist to target 0.3429005856528189
episode reward for DDPG agent: -0.8202774822711945
steps: 19975, episodes: 800, mean episode reward: -4.457159214486486, time: 6.448
good reward is 8.77870315567302
Avg best coop dist to target 0.3393191581825757
episode reward for DDPG agent: -0.7088817203044891
steps: 24975, episodes: 1000, mean episode reward: -3.544541959292395, time: 6.321
good reward is 10.005020282042326
Avg best coop dist to target 0.3294734921069317
episode reward for DDPG agent: -0.49480643540620806
steps: 29975, episodes: 1200, mean episode reward: -6.013959131490217, time: 9.86
good reward is 7.126626042561898
Avg best coop dist to target 0.33860413776931436
episode reward for DDPG agent: -0.8161654549837113
steps: 34975, episodes: 1400, mean episode reward: -1.9962852274804936, time: 9.219
good reward is 10.28365704807453
Avg best coop dist to target 0.326327640384892
final is 8.666940572966451
Traceback (most recent call last):
  File "C:/Users/liut18/Desktop/maddpg/experiments/train_DDPG_Adv_Policy_PD_Blackbox.py", line 386, in <module>
    train(arglist)
  File "C:/Users/liut18/Desktop/maddpg/experiments/train_DDPG_Adv_Policy_PD_Blackbox.py", line 374, in train
    rew_file_name = arglist.plots_dir + arglist.exp_name + '_rewards.pkl'
TypeError: must be str, not NoneType