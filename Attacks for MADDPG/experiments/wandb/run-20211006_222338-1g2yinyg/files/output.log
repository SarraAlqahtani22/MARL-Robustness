Using good policy maddpg and adv policy maddpg
Loading previous state...
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\maddpg\common\tf_util.py:163: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\maddpg\trainer\maddpg.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\contrib\layers\python\layers\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\python\ops\clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
## New start!
loading-------------------------------------------------------------
Starting iterations...
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\experiments\algo\airl_discriminator.py:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\experiments\algo\airl_discriminator.py:26: The name tf.losses.log_loss is deprecated. Please use tf.compat.v1.losses.log_loss instead.
WARNING:tensorflow:From C:\Users\liut18\Desktop\maddpg\venv\lib\site-packages\tensorflow_core\python\training\moving_averages.py:434: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
episode reward for DDPG agent: -1.6092926809191703
steps: 4975, episodes: 200, mean episode reward: 2.624508893983097, time: 7.192
good reward is 15.407743532716887
Avg best coop dist to target 0.18390696959624322
episode reward for DDPG agent: -1.5825877803564072
steps: 9975, episodes: 400, mean episode reward: 2.0933572031004593, time: 6.163
good reward is 14.815225682359923
Avg best coop dist to target 0.18631772347276135
episode reward for DDPG agent: -1.5359041801095008
steps: 14975, episodes: 600, mean episode reward: 3.5872443451452773, time: 6.228
good reward is 16.125417499456024
Avg best coop dist to target 0.17423489486982557
episode reward for DDPG agent: -1.3747153306007385
steps: 19975, episodes: 800, mean episode reward: 3.3363291084449282, time: 6.214
good reward is 16.55955554178096
Avg best coop dist to target 0.1715671149899434
episode reward for DDPG agent: -1.6992972975969314
steps: 24975, episodes: 1000, mean episode reward: 3.4159482325276866, time: 6.335
good reward is 16.991286517501496
Avg best coop dist to target 0.16805469259040595
episode reward for DDPG agent: -1.5449981153011323
steps: 29975, episodes: 1200, mean episode reward: 2.0162441575590377, time: 9.864
good reward is 15.475422857601139
Avg best coop dist to target 0.1691330983413082
episode reward for DDPG agent: -2.3564408773183825
steps: 34975, episodes: 1400, mean episode reward: 3.0034178322396183, time: 9.02
good reward is 16.305697912913942
Avg best coop dist to target 0.1664513960256604
final is 15.954335649190055
Traceback (most recent call last):
  File "C:/Users/liut18/Desktop/maddpg/experiments/train_DDPG_Adv_Policy_PD_Blackbox.py", line 386, in <module>
    train(arglist)
  File "C:/Users/liut18/Desktop/maddpg/experiments/train_DDPG_Adv_Policy_PD_Blackbox.py", line 374, in train
    rew_file_name = arglist.plots_dir + arglist.exp_name + '_rewards.pkl'
TypeError: must be str, not NoneType